# ä¸€ã€ åŸºæœ¬ç±»ä»‹ç»

## 1. Mapperå‡½æ•°

### 1.1 å‡½æ•°è¦æ±‚

-  ç»§æ‰¿Hadoopçš„Mapperç±»

- é‡å†™mapæ–¹æ³•

### 1.2 å‡½æ•°ä½œç”¨

- è¯»å–åŸå§‹æ•°æ®ï¼Œå°†è¾“å…¥æ•°æ®ä»¥é”®å€¼å¯¹çš„å½¢å¼è¾“å‡º
- ä½¿åŸå§‹çš„æ— ç»“æ„çš„æ•°æ®å˜æˆæœ‰ç»“æ„çš„æ•°æ®

## 2. Reducerå‡½æ•°

### 2.1 å‡½æ•°è¦æ±‚

- ç»§æ‰¿Hadoopçš„Reducerç±»
- é‡å†™reduceæ–¹æ³•

### 2.2 å‡½æ•°ä½œç”¨

- ä»mapperä¸­è¯»å–æ•°æ®
- å¯¹æ•°æ®è¿›è¡Œæ•´ç†ï¼Œä½œä¸ºæœ€ç»ˆçš„è¾“å‡ºæ•°æ®

## 3. Driverå‡½æ•°

### 3.1 å‡½æ•°è¦æ±‚

- è·å–jobå¯¹è±¡
- é…ç½®jarç±»ï¼Œé…ç½®mapperç±»ï¼Œreducerç±»
- æŒ‡å®šmapperè¾“å…¥è¾“å‡ºç±»ï¼ŒæŒ‡å®šæœ€ç»ˆç»“æœçš„è¾“å…¥è¾“å‡ºç±»
- æäº¤

### 3.2 å‡½æ•°ä½œç”¨

ç›¸å½“äºé…ç½®äº†yarnçš„å®¢æˆ·ç«¯ï¼Œç”¨äºæäº¤æˆ‘ä»¬æ•´ä¸ªä½œä¸šåˆ°yarné›†ç¾¤



# äºŒã€ WordCountï¼šç»Ÿè®¡å•è¯æ•°

## 1. å®éªŒä»‹ç»

### 1.1 å®éªŒè¾“å…¥

```
atguigu atguigu
ss ss
cls cls
jiao
banzhang
xue
hadoop
```



### 1.2 æœŸå¾…è¾“å‡º

```
atguigu	2
banzhang	1
cls	
hadoop	1
jiao	1
ss	2
xue	1
```



### 1.3 å®éªŒè¯´æ˜

è¯»å–æŒ‡å®šæ–‡ä»¶ï¼Œç»Ÿè®¡æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°

## 2. å®éªŒä»£ç 

### 2.1 Mapperç±»

1. ç±»ä»‹ç»ï¼š

   ä»è¾“å…¥æ–‡ä»¶ä¸­ä¸€è¡Œä¸€è¡Œçš„è¯»å–æ•°æ®ï¼Œç„¶åä»¥é”®å€¼å¯¹çš„å½¢å¼è¾“å‡ºåˆ°reducerä¸­ã€‚æ¯”å¦‚ï¼šè¯»å–åˆ°ç¬¬ä¸€ä¸ªatguiguçš„æ—¶å€™ï¼Œä¼šç”Ÿæˆä¸€ä¸ª<atguigu,1>çš„é”®å€¼å¯¹è¾“å‡ºã€‚åœ¨mapperé˜¶æ®µä¼šå¹¶è¡Œçš„ç”Ÿæˆå¾ˆå¤šä¸ªé”®å€¼å¯¹è¾“å‡ºæ•°æ®ã€‚

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.wordcount;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.IntWritable;
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Mapper;
   
   public class WordcountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
   	
   	Text k=new Text();
   	IntWritable v=new IntWritable(1);
   
   	@Override
   	protected void map(LongWritable key, Text value, Context context)
   			throws IOException, InterruptedException {
   		
   		// get a line
   		String line=value.toString();
   		
   		// split by space
   		String[] words=line.split(" ");
   		
   		// output
   		for (String word : words) {
   			k.set(word);
   			context.write(k, v);
   		}
   	}
   }
   
   ```

### 2.2 Reducerç±»

1. ç±»ä»‹ç»ï¼š

   Mapé˜¶æ®µä¼šç”Ÿæˆå¾ˆå¤šé”®å€¼å¯¹æ•°æ®ï¼Œè¿™äº›é”®å€¼å¯¹åœ¨ç»è¿‡ä¸€ä¸ªshuffleé˜¶æ®µæ’åºåï¼Œä¼šè¾“å‡ºåˆ°reducerã€‚å¹¶ä¸”ç›¸åŒkeyçš„æ•°æ®ä¼šè¢«å°è£…åˆ°ä¸€èµ·ã€‚åŠ å…¥mapè¾“å‡ºçš„é”®å€¼å¯¹ä¿¡æ¯å¦‚ä¸‹æ‰€ç¤ºï¼š<atguigu,1>ï¼Œ<ss,1>ï¼Œ<atguigu,1>é‚£ä¹ˆreducerä¼šè·å–åˆ°çš„è¾“å…¥å¦‚ä¸‹ï¼š<atguigu,[1,1]>ï¼Œ<ss,1>ã€‚æ‰€ä»¥reducerè·å–åˆ°çš„é”®å€¼å¯¹å½¢å¼ä¸º<key, values[â€¦]>

   å¯¹æ¯ä¸ªkeyï¼Œrecuderä¼šå–å‡ºå®ƒçš„valuesä¿¡æ¯ã€‚ä»è€Œè¿›è¡Œåé¢çš„æ“ä½œ

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.wordcount;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.IntWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   
   public class WordcountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
   
   	int sum;
   	IntWritable v = new IntWritable();
   
   	@Override
   	protected void reduce(Text key, Iterable<IntWritable> values, Context context)
   			throws IOException, InterruptedException {
   
   		// 1. sum the values
   		sum = 0;
   		for (IntWritable count : values) {
   			sum += count.get();
   		}
   
   		// 2. output the result
   		v.set(sum);
   		context.write(key, v);
   	}
   
   }
   
   ```

### 2.3 Driverç±»

1. ç±»ä»‹ç»ï¼š

   å°±æ˜¯æ­£å¸¸çš„Driverç±»ï¼Œè·å–jobå¯¹è±¡ä¹‹åï¼Œè¿™æ˜¯ç›¸åº”çš„ä¿¡æ¯ï¼Œç„¶åæäº¤

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.wordcount;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.IntWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   
   /**
    * Driver will configure yarn
    * 
    * 1. Get job object
    * 
    * 2. Specify jar class, Mapper class and Reducer class (3)
    * 
    * 3. Specify Mapper input and output class and Final input and output class (2)
    * 
    * 4. Submit (1)
    */
   public class WordcountDriver {
   
   	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
   
           args = new String[] { "e:/input/wordcount", "e:/output/wordcount" };
   		// 1. Obtain configuration information and packaging tasks
   		Configuration conf = new Configuration();
   		Job job = Job.getInstance(conf);
   
   		// 2. Set jar loading path
   		job.setJarByClass(WordcountDriver.class);
   
   		// 3. set map and reduce class
   		job.setMapperClass(WordcountMapper.class);
   		job.setReducerClass(WordcountReducer.class);
   
   		// 4. set map output
   		job.setMapOutputKeyClass(Text.class);
   		job.setOutputValueClass(IntWritable.class);
   
   		// 5. set final k and v output type
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(IntWritable.class);
   
   		// 6. set input path and output path
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 7. submit
   		boolean result = job.waitForCompletion(true);
   		System.exit(result ? 0 : 1);
   	}
   
   }
   ```



# ä¸‰ã€ FlowSumï¼šç»Ÿè®¡æ‰‹æœºæµé‡

## 1. å®éªŒä»‹ç»

### 1.1 å®éªŒè¾“å…¥

```
1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
```



### 1.2 æœŸå¾…è¾“å‡º

```
13470253144	180	180	360
13509468723	7335	110349	117684
13560439638	918	4938	5856
13568436656	3597	25635	29232
13590439668	1116	954	2070
13630577991	6960	690	7650
13682846555	1938	2910	4848
13729199489	240	0	240
13736230513	2481	24681	27162
13768778790	120	120	240
13846544121	264	0	264
13956435636	132	1512	1644
13966251146	240	0	240
13975057813	11058	48243	59301
13992314666	3008	3720	6728
15043685818	3659	3538	7197
15910133277	3156	2936	6092
15959002129	1938	180	2118
18271575951	1527	2106	3633
18390173782	9531	2412	11943
84188413	4116	1432	5548
```



### 1.3 å®éªŒè¯´æ˜

è¯»å–æ‰‹æœºçš„æµé‡ä¿¡æ¯ï¼Œç„¶åè¾“å‡ºæ‰‹æœºçš„ä¸Šè¡Œæµé‡ã€ä¸‹è¡Œæµé‡å’Œæ€»æµé‡

ç¼–å†™FlowBeanç±»ï¼Œå®ç°æ‰‹æœºæµé‡çš„åºåˆ—åŒ–ï¼šä½¿ç”¨Hadoopæ–¹å¼çš„åºåˆ—åŒ–è€Œä¸æ˜¯Javaçš„åºåˆ—åŒ–ï¼Œæœ‰å¾ˆå¤šçš„å¥½å¤„ã€‚

## 2. å®éªŒä»£ç 

### 2.1 FlowBeanç±»

1. ç±»è¯´æ˜ï¼š

   - ç¼–å†™FlowBeanç±»ä¸»è¦ä½¿ä¸ºäº†å®ç°åºåˆ—åŒ–ã€‚åºåˆ—åŒ–ä½¿ä¸ºäº†ä½¿æˆ‘ä»¬çš„è‡ªå®šä¹‰æ•°æ®ç±»å‹å¯ä»¥è‡ªç”±çš„ä¼ è¾“ï¼Œæ¯”å¦‚åœ¨<key, value>ä¸­ä¼ è¾“
   - å®ç°Writableæ¥å£
   - æä¾›ç©ºå‚å‡½æ•°ä¾›ååºåˆ—åŒ–æ—¶ä½¿ç”¨
   - é‡å†™åºåˆ—åŒ–æ–¹æ³•å’Œååºåˆ—åŒ–æ–¹æ³•ï¼Œå‘åºåˆ—åŒ–çš„é¡ºåºå¿…é¡»å’Œåºåˆ—åŒ–çš„ä¸€è‡´
   - é‡å†™toStringæ–¹æ³•ä»¥æŠŠç»“æœæ˜¾ç¤ºåœ¨æ–‡ä»¶ä¸­
   - å®ç°comparableæ¥å£ï¼Œæ¥ä½¿FlowBeanå¯ä»¥æ”¾åˆ°<key, value>ä¸­ä¼ æ’­ã€‚å› ä¸ºæˆ‘ä»¬å¿…é¡»ä¸ºshuffleæä¾›æ¯”è¾ƒçš„ä¾æ®

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.flowsum;
   
   import java.io.DataInput;
   import java.io.DataOutput;
   import java.io.IOException;
   
   import org.apache.hadoop.io.Writable;
   
   // 1. Implement the writable interface
   public class FlowBean implements Writable {
   
   	private long upFlow;
   	private long downFlow;
   	private long sumFlow;
   
   	// When deserializing, you need to call the empty parameter constructor, so you
   	// must have
   	public FlowBean() {
   		super();
   	}
   
   	public FlowBean(long upFlow, long downFlow) {
   		super();
   		this.upFlow = upFlow;
   		this.downFlow = downFlow;
   		this.sumFlow = upFlow + downFlow;
   	}
   
   	// Write serialization method
   	public void write(DataOutput out) throws IOException {
   		out.writeLong(upFlow);
   		out.writeLong(downFlow);
   		out.writeLong(sumFlow);
   	}
   
   	// Deserialization method
   	// The read sequence of the deserialization method must be the same as the write
   	// sequence of the write serialization method
   	public void readFields(DataInput in) throws IOException {
   		this.upFlow = in.readLong();
   		this.downFlow = in.readLong();
   		this.sumFlow = in.readLong();
   	}
   
   	// Write toString method to facilitate subsequent printing to text
   	@Override
   	public String toString() {
   		return upFlow + "\t" + downFlow + "\t" + sumFlow;
   	}
   
   	// set and get methods
   	public long getUpFlow() {
   		return upFlow;
   	}
   
   	public void setUpFlow(long upFlow) {
   		this.upFlow = upFlow;
   	}
   
   	public long getDownFlow() {
   		return downFlow;
   	}
   
   	public void setDownFlow(long downFlow) {
   		this.downFlow = downFlow;
   	}
   
   	public long getSumFlow() {
   		return sumFlow;
   	}
   
   	public void setSumFlow(long sumFlow) {
   		this.sumFlow = sumFlow;
   	}
   
   	public void set(long upFlow2, long downFlow2) {
   		this.upFlow = upFlow2;
   		this.downFlow = downFlow2;
   		this.sumFlow = upFlow2 + downFlow2;
   	}
   }
   ```

### 2.2 Mapperç±»

1. ç±»è¯´æ˜ï¼š
   
   ä»æ–‡ä»¶é‡Œè¯»å–æ•°æ®ï¼Œç„¶åå°è£…åˆ°FowBeanå¯¹è±¡ä¸­ï¼Œä¼ é€’åˆ°Reducerä¸­
   
2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.flowsum;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Mapper;
   
   public class FlowCountMapper extends Mapper<LongWritable, Text, Text, FlowBean> {
   
   	// 1 13736230513 192.196.100.1 www.atguigu.com 2481 24681 200
   	FlowBean v = new FlowBean();
   	Text k = new Text();
   
   	@Override
   	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
   
   		// 1. get a row
   		String line = value.toString();
   
   		// 2. cut field
   		String[] fields = line.split("\t");
   
   		// 3. package object,get phone number
   		String phoneNum = fields[1];
   
   		// 4. get upFlow and downFlow
   		long upFlow = Long.parseLong(fields[fields.length - 3]);
   		long downFlow = Long.parseLong(fields[fields.length - 2]);
   		k.set(phoneNum);
   		v.set(upFlow, downFlow);
   
   		// 5. write out
   		context.write(k, v);
   	}
   }
   
   ```

   

### 2.3 Reducerç±»

1. ç±»è¯´æ˜ï¼š

   ä»Mapperä¸­è¯»å–FlowBeanï¼Œç„¶åæ±‡æ€»ä¸Šè¡Œæµé‡å’Œä¸‹è¡Œæµé‡

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.flowsum;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   
   public class FlowCountReducer extends Reducer<Text, FlowBean, Text, FlowBean> {
   	@Override
   	protected void reduce(Text key, Iterable<FlowBean> values, Context content)
   			throws IOException, InterruptedException {
   		long sum_upFlow = 0;
   		long sum_downFlow = 0;
   
   		// 1. Traverse the used beans and accumulate the upstream traffic and downstream
   		// traffic separately
   		for (FlowBean flowBean : values) {
   			sum_upFlow += flowBean.getUpFlow();
   			sum_downFlow += flowBean.getDownFlow();
   		}
   
   		// 2. Package object
   		FlowBean resutlBean = new FlowBean(sum_upFlow, sum_downFlow);
   
   		// 3. Write out
   		content.write(key, resutlBean);
   	}
   }
   
   ```

   

### 2.4 Driverç±»

1. ç±»è¯´æ˜ï¼šå°±æ˜¯æ­£å¸¸çš„Driverç±»ï¼Œè·å–jobå¯¹è±¡ä¹‹åï¼Œè¿™æ˜¯ç›¸åº”çš„ä¿¡æ¯ï¼Œç„¶åæäº¤

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.flowsum;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   
   public class FlowsumDriver {
   
   	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
   		// Set input path and output path
   		args = new String[] { "e:/input/flowSum", "e:/output/flowSum" };
   
   		// 1. Get configuration information, or job object instance
   		Configuration conf = new Configuration();
   		Job job = Job.getInstance(conf);
   
   		// 2. Specify the local path where the jar package of this program is located
   		job.setJarByClass(FlowsumDriver.class);
   
   		// 3. Specify the mapper class and the reducer class
   		job.setMapperClass(FlowCountMapper.class);
   		job.setReducerClass(FlowCountReducer.class);
   
   		// 4. Specify map output key type and value type
   		job.setMapOutputKeyClass(Text.class);
   		job.setMapOutputValueClass(FlowBean.class);
   
   		// 5. Specify final output key type and value type
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(FlowBean.class);
   
   		// 6. Specify the directory where the original input file of the job is located
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 7. Submit
   		Boolean result = job.waitForCompletion(true);
   		System.exit(result ? 0 : 1);
   	}
   }
   ```



# å››ã€ InputFormatï¼šå¹¶è¡Œåº¦å†³å®šæœºåˆ¶

CombineTextInputFormatæœºåˆ¶ï¼šå°†åŸæœ¬åˆ’åˆ†ä¸ºä¸€ä¸ªå•ç‹¬åˆ‡ç‰‡çš„å°æ–‡ä»¶æ±‡æ€»èµ·æ¥ä¸ºä¸€ä¸ªå¤§çš„æ–‡ä»¶ï¼Œè¿™æ ·å¯ä»¥é¿å…å°æ–‡ä»¶è¿‡å¤šé€ æˆçš„æ€§èƒ½æµªè´¹ã€‚

ç”Ÿæˆåˆ‡ç‰‡çš„è¿‡ç¨‹ï¼šè™šæ‹Ÿå­˜å‚¨è¿‡ç¨‹+åˆ‡ç‰‡è¿‡ç¨‹

FileInputFormatå®ç°ç±»ï¼š

1. TextInputFormatï¼šå®ƒä½¿é»˜è®¤çš„FileInputFormatå®ç°ç±»ï¼Œå®ƒæŒ‰è¡Œè¯»å–æ¯è¡Œæ¶ˆæ¯ï¼Œkeyå€¼æ˜¯æ¯è¡Œèµ·å§‹å­—èŠ‚çš„åç§»é‡ï¼Œvalueå€¼æ˜¯è¿™è¡Œçš„å†…å®¹
2. KeyValueTextInputFormatï¼šæ¯è¡Œå‡ä¸ºä¸€æ¡è®°å½•ï¼Œkeyå€¼å’Œvalueå€¼ç”±åˆ†éš”ç¬¦åˆ†éš”å¼€
3. NLineInputFormatï¼šæ¯Nè¡Œåˆ’åˆ†ä¸ºä¸€ä¸ªmapperè¿›ç¨‹
4. CombineTextInputFormatï¼š



## 1. KeyValueInputFormatæ¡ˆä¾‹

### 1.1 å®éªŒä»‹ç»

#### 1.1.1 å®éªŒè¾“å…¥

```
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
```

#### 1.1.2 æœŸå¾…è¾“å‡º

```
banzhang	2
xihuan	2
```

#### 1.1.3 å®éªŒè¯´æ˜

ç»Ÿè®¡è¾“å…¥æ–‡ä»¶ä¸­æ¯ä¸€è¡Œçš„ç¬¬ä¸€ä¸ªå•è¯ç›¸åŒçš„è¡Œæ•°ã€‚æˆ‘ä»¬æŒ‡å®šç©ºæ ¼ä¸ºåˆ†éš”ç¬¦ï¼Œè¿™æ ·æ¯è¡Œç¬¬ä¸€ä¸ªå•è¯å°±ä½œä¸ºkeyã€‚

### 1.2 å®éªŒä»£ç 

#### 1.2.1 Mapperç±»

1. ç±»è¯´æ˜ï¼šæˆ‘ä»¬æŒ‡å®švalueä¸º1ï¼Œè¿™æ ·æ¯æ¬¡è¯»å–åˆ°åŒæ ·çš„å•è¯ï¼Œå°±å¯ä»¥+1

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.KeyValueTextInputFormat;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Mapper;
   
   public class KVTextMapper extends Mapper<Text, Text, Text, LongWritable> {
   
   	// 1 è®¾ç½®value
   	LongWritable v = new LongWritable(1);
   
   	@Override
   	protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
   
   		// banzhang ni hao
   
   		// 2 å†™å‡º
   		context.write(key, v);
   	}
   }
   ```

   

#### 1.2.2 Reducerç±»

1. ç±»è¯´æ˜ï¼šå¯¹Mapperä¼ æ¥çš„æ•°æ®è¿›è¡Œæ±‡æ€»+1å³å¯

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.KeyValueTextInputFormat;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   
   public class KVTextReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
   
   	LongWritable v = new LongWritable();
   
   	@Override
   	protected void reduce(Text key, Iterable<LongWritable> values, Context context)
   			throws IOException, InterruptedException {
   
   		long sum = 0L;
   
   		// 1 æ±‡æ€»ç»Ÿè®¡
   		for (LongWritable value : values) {
   			sum += value.get();
   		}
   
   		v.set(sum);
   
   		// 2 è¾“å‡º
   		context.write(key, v);
   	}
   }
   ```

   

#### 1.2.3 Driverç±»

1. ç±»è¯´æ˜ï¼š

   - æŒ‡å®šåˆ†éš”ç¬¦ä¸ºç©ºæ ¼ï¼šconf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, " ")
   - ä½¿ç”¨KeyValueTextInputFormatå¤„ç†ï¼šjob.setInputFormatClass(KeyValueTextInputFormat.class);

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.KeyValueTextInputFormat;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;
   import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   
   public class KVTextDriver {
   
   	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
   
   		args = new String[] { "e:/input/kvtInput", "e:/output/kvtOutput" };
   		Configuration conf = new Configuration();
   		// è®¾ç½®åˆ‡å‰²ç¬¦
   		conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, " ");
   		// 1 è·å–jobå¯¹è±¡
   		Job job = Job.getInstance(conf);
   
   		// 2 è®¾ç½®jaråŒ…ä½ç½®ï¼Œå…³è”mapperå’Œreducer
   		job.setJarByClass(KVTextDriver.class);
   		job.setMapperClass(KVTextMapper.class);
   		job.setReducerClass(KVTextReducer.class);
   
   		// 3 è®¾ç½®mapè¾“å‡ºkvç±»å‹
   		job.setMapOutputKeyClass(Text.class);
   		job.setMapOutputValueClass(LongWritable.class);
   
   		// 4 è®¾ç½®æœ€ç»ˆè¾“å‡ºkvç±»å‹
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(LongWritable.class);
   
   		// 5 è®¾ç½®è¾“å…¥è¾“å‡ºæ•°æ®è·¯å¾„
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   
   		// è®¾ç½®è¾“å…¥æ ¼å¼ï¼Œâ€»
   		job.setInputFormatClass(KeyValueTextInputFormat.class);
   
   		// 6 è®¾ç½®è¾“å‡ºæ•°æ®è·¯å¾„
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 7 æäº¤job
   		job.waitForCompletion(true);
   	}
   }
   ```

   

## 2. NLineInputFormatæ¡ˆä¾‹

### 2.1 å®éªŒä»‹ç»

#### 2.1.1 å®éªŒè¾“å…¥

```
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang banzhang ni hao
xihuan hadoop banzhang
```



#### 2.1.2 æœŸå¾…è¾“å‡º

```
banzhang	12
hadoop	6
hao	6
ni	6
xihuan	6
```

æœŸå¾…åœ¨Eclipseçš„æ§åˆ¶å°ä¸­çœ‹åˆ°ï¼š**Number of splits:4**

#### 2.1.3 å®éªŒè¯´æ˜

å¯¹æ¯ä¸ªå•è¯è¿›è¡Œä¸ªæ•°ç»Ÿè®¡ï¼Œè¦æ±‚æ ¹æ®æ¯ä¸ªè¾“å…¥æ–‡ä»¶çš„è¡Œæ•°æ¥è§„å®šè¾“å‡ºå¤šå°‘ä¸ªåˆ‡ç‰‡ã€‚æ­¤æ¡ˆä¾‹è¦æ±‚æ¯ä¸‰è¡Œæ”¾å…¥ä¸€ä¸ªåˆ‡ç‰‡ä¸­ã€‚

### 2.2 å®éªŒä»£ç 

#### 2.2.1 Mapperç±»

1. ç±»è¯´æ˜ï¼šå¯¹æ¯ä¸ªå•è¯è¾“å‡ºä¸º<key, value>åˆ°mapperä¸­

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.nline;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Mapper;
   
   /**
    * read file by rows, output like <banzhang,1> <ni,1> <hao,1>
    */
   public class NLineMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
   
   //	banzhang ni hao
   //	xihuan hadoop banzhang
   //	banzhang ni hao
   	private Text k = new Text();
   	private LongWritable v = new LongWritable(1);
   
   	@Override
   	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
   		// 1. Get a row
   		String line = value.toString();
   
   		// 2. Split
   		String[] splited = line.split(" ");
   
   		// 3. Write out
   		for (int i = 0; i < splited.length; i++) {
   			k.set(splited[i]);
   			context.write(k, v);
   		}
   	}
   }
   ```

   

#### 2.2.2 Reducerç±»

1. ç±»è¯´æ˜ï¼šå¯¹mapperä¼ æ¥çš„å€¼è¿›è¡Œæ±‡æ€»+1

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.nline;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   
   public class NLineReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
   
   	LongWritable v = new LongWritable();
   
   	@Override
   	protected void reduce(Text key, Iterable<LongWritable> values, Context content)
   			throws IOException, InterruptedException {
   
   		long sum = 0l;
   
   		// 1. Generate all data
   		for (LongWritable value : values) {
   			sum += value.get();
   		}
   		v.set(sum);
   
   		// 2. Write out
   		content.write(key, v);
   	}
   }
   ```

   

#### 2.2.3 Driverç±»

1. ç±»è¯´æ˜ï¼š

   - è®¾ç½®æ¯ä¸ªåˆ‡ç‰‡InputSplitä¸­åˆ’åˆ†ä¸‰æ¡è®°å½•ï¼šNLineInputFormat.setNumLinesPerSplit(job, 3);
   - ä½¿ç”¨NLineInputFormatæ¥å¤„ç†ï¼šjob.setInputFormatClass(NLineInputFormat.class);  

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.nline;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   
   public class NLineDriver {
   
   	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
   
   		args = new String[] { "e:/input/nLineInput", "e:/output/nlineOutput" };
   
   		// 1. Get job object
   		Configuration conf = new Configuration();
   		Job job = Job.getInstance(conf);
   
   		// 7è®¾ç½®æ¯ä¸ªåˆ‡ç‰‡InputSplitä¸­åˆ’åˆ†ä¸‰æ¡è®°å½•
   		NLineInputFormat.setNumLinesPerSplit(job, 3);
   
   		// 8ä½¿ç”¨NLineInputFormatå¤„ç†è®°å½•æ•°
   		job.setInputFormatClass(NLineInputFormat.class);
   
   		// 2è®¾ç½®jaråŒ…ä½ç½®ï¼Œå…³è”mapperå’Œreducer
   		job.setJarByClass(NLineDriver.class);
   		job.setMapperClass(NLineMapper.class);
   		job.setReducerClass(NLineReducer.class);
   
   		// 3è®¾ç½®mapè¾“å‡ºkvç±»å‹
   		job.setMapOutputKeyClass(Text.class);
   		job.setMapOutputValueClass(LongWritable.class);
   
   		// 4è®¾ç½®æœ€ç»ˆè¾“å‡ºkvç±»å‹
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(LongWritable.class);
   
   		// 5è®¾ç½®è¾“å…¥è¾“å‡ºæ•°æ®è·¯å¾„
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 6æäº¤job
   		job.waitForCompletion(true);
   
   	}
   }
   ```

   å¯ä»¥çœ‹åˆ°å¦‚ä¸‹ç»“æœï¼š

   ![image-20200419105719141.png](https://github.com/simplefujie/EasyHadoopProject/blob/master/studyNotes/02-mapreduce-experiments.assets/image-20200419105719141.png)

## 3. è‡ªå®šä¹‰InputFormatæ¡ˆä¾‹

### 3.1 è¯•éªŒä»‹ç»

#### 3.1.1 å®éªŒè¾“å…¥

one.txt

```
yongpeng weidong weinan
sanfeng luozong xiaoming
```

two.txt

```
longlong fanfan
mazong kailun yuhang yixin
longlong fanfan
mazong kailun yuhang yixin
```

three.txt

```
shuaige changmo zhenqiang 
dongli lingu xuanxuan
```



#### 3.1.2 æœŸå¾…è¾“å‡º

```
SEQorg.apache.hadoop.io.Text"org.apache.hadoop.io.BytesWritable      ?8ré·‘æ’¾??60   ]   ('file:/e:/input/inputFormatInput/one.txt   1yongpeng weidong weinan
sanfeng luozong xiaoming   _   *)file:/e:/input/inputFormatInput/three.txt   1shuaige changmo zhenqiang 
dongli lingu xuanxuan   ?   ('file:/e:/input/inputFormatInput/two.txt   Xlonglong fanfan
mazong kailun yuhang yixin
longlong fanfan
mazong kailun yuhang yixin
```

æœŸæœ›è¾“å‡ºæ–‡ä»¶æ ¼å¼ï¼š**part-r-00000**

#### 3.1.3 å®éªŒè¯´æ˜

- Qï¼šä¸ºä»€ä¹ˆè¦è‡ªå®šä¹‰InputFormat

  Aï¼šåœ¨ä¼ä¸šå¼€å‘ä¸­ï¼ŒHadoopè‡ªå¸¦çš„inputformatä¸èƒ½æ»¡è¶³åº”ç”¨åœºæ™¯ï¼Œéœ€è¦ä½¿ç”¨è‡ªå®šä¹‰çš„

- Qï¼šè‡ªå®šä¹‰InputFormatæ­¥éª¤ï¼š

  Aï¼šè‡ªå®šä¹‰æ­¥éª¤å¦‚ä¸‹æ‰€ç¤º

  1. è‡ªå®šä¹‰ä¸€ä¸ªç±»ç»§æ‰¿FileInputFormat
  2. æ”¹å†™RecordReaderå®ç°ä¸€æ¬¡æ€§è¯»å–ä¸€ä¸ªå®Œæ•´æ–‡ä»¶ä¸ºKV
  3. è¾“å‡ºæ—¶ä½¿ç”¨SequenceFileOutputFormatè¾“å‡ºåˆå¹¶æ–‡ä»¶

å°†å¤šä¸ªå°æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªSequenceFileæ–‡ä»¶ï¼ˆSequenceFileæ–‡ä»¶æ˜¯Hadoopç”¨æ¥å­˜å‚¨äºŒè¿›åˆ¶å½¢å¼çš„key-valueå¯¹çš„æ–‡ä»¶æ ¼å¼ï¼‰ï¼ŒSequenceFileé‡Œé¢å­˜å‚¨ç€å¤šä¸ªæ–‡ä»¶ï¼Œå­˜å‚¨çš„å½¢å¼ä¸ºæ–‡ä»¶è·¯å¾„+åç§°ä¸ºkeyï¼Œæ–‡ä»¶å†…å®¹ä¸ºvalueã€‚éœ€æ±‚åˆ†æå¦‚ä¸‹ï¼š

1. è‡ªå®šä¹‰ä¸€ä¸ªç±»ç»§æ‰¿ï¼šFileInputFormat
   - é‡å†™isSplitable()æ–¹æ³•ï¼Œè¿”å›falseè¡¨ç¤ºä¸å¯åˆ‡å‰²
   - é‡å†™createRecordReader()ï¼Œåˆ›å»ºè‡ªå®šä¹‰çš„RecordReaderå¯¹è±¡ï¼Œå¹¶åˆå§‹åŒ–
2. æ”¹å†™RecordReaderï¼Œå®ç°ä¸€æ¬¡è¯»å–ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶å°è£…ä¸ºKV
   - é‡‡ç”¨äº†IOæµä¸€æ¬¡è¯»å–ä¸€ä¸ªæ–‡ä»¶è¾“å‡ºåˆ°valueä¸­
   - è·å–æ–‡ä»¶è·¯å¾„ä¿¡æ¯+åç§°ï¼Œå¹¶è®¾ç½®key
3. è®¾ç½®Driver
   - è®¾ç½®è¾“å…¥çš„inputFormatç±»å‹ï¼šjob.setInputFormatClass(WholeFileInputformat.class);
   - è®¾ç½®è¾“å‡ºçš„outputFormatç±»å‹ï¼šjob.setOutputFormatClass(SequenceFileOutputFormat.class);



### 3.2 å®éªŒä»£ç 

#### 3.2.1 WholeRecordReaderç±»

1. ç±»è¯´æ˜ï¼šæ¯å½“è¯»å–ä¸€ä¸ªæ–‡ä»¶çš„æ—¶å€™ï¼Œä»¥è¯¥æ–‡ä»¶çš„è·¯å¾„+åå­—ä½œä¸ºKeyï¼Œä»¥æ–‡ä»¶å†…å®¹ä½œä¸ºvalue

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.inputformat;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.FSDataInputStream;
   import org.apache.hadoop.fs.FileSystem;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.BytesWritable;
   import org.apache.hadoop.io.IOUtils;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.InputSplit;
   import org.apache.hadoop.mapreduce.RecordReader;
   import org.apache.hadoop.mapreduce.TaskAttemptContext;
   import org.apache.hadoop.mapreduce.lib.input.FileSplit;
   
   public class WholeRecordReader extends RecordReader<Text, BytesWritable> {
   
   	private Configuration configuration;
   	private FileSplit split;
   
   	private boolean isProgress = true;
   	private BytesWritable value = new BytesWritable();
   	private Text k = new Text();
   
   	@Override
   	public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {
   
   		this.split = (FileSplit) split;
   		configuration = context.getConfiguration();
   	}
   
   	@Override
   	public boolean nextKeyValue() throws IOException, InterruptedException {
   		if (isProgress) {
   			// 1. Define buffer
   			byte[] contents = new byte[(int) split.getLength()];
   			FileSystem fs = null;
   			FSDataInputStream fis = null;
   
   			try {
   				// 2. Get file system
   				Path path = split.getPath();
   				fs = path.getFileSystem(configuration);
   
   				// 3. Read data
   				fis = fs.open(path);
   
   				// 4. Get file content
   				IOUtils.readFully(fis, contents, 0, contents.length);
   
   				// 5. Output file content
   				value.set(contents, 0, contents.length);
   
   				// 6. Get file path and file name
   				String name = split.getPath().toString();
   
   				// 7. Set output key
   				k.set(name);
   			} catch (Exception e) {
   				// TODO: handle exception
   			} finally {
   				IOUtils.closeStream(fis);
   			}
   			isProgress = false;
   			return true;
   		}
   		return false;
   	}
   
   	@Override
   	public Text getCurrentKey() throws IOException, InterruptedException {
   		return k;
   	}
   
   	@Override
   	public BytesWritable getCurrentValue() throws IOException, InterruptedException {
   		return value;
   	}
   
   	@Override
   	public float getProgress() throws IOException, InterruptedException {
   		return 0;
   	}
   
   	@Override
   	public void close() throws IOException {
   	}
   
   }
   ```

   

#### 3.2.2 WholeFileInputFormatç±»

1. ç±»è¯´æ˜ï¼šåˆå§‹åŒ–å¹¶ä¸”è¿”å›æˆ‘ä»¬è‡ªå®šä¹‰çš„WholeRecordReaderå¯¹è±¡

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.inputformat;
   
   import java.io.IOException;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.BytesWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.InputSplit;
   import org.apache.hadoop.mapreduce.JobContext;
   import org.apache.hadoop.mapreduce.RecordReader;
   import org.apache.hadoop.mapreduce.TaskAttemptContext;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   
   // å®šä¹‰ç±»ç»§æ‰¿FileInputFormat
   public class WholeFileInputformat extends FileInputFormat<Text, BytesWritable> {
   
   	@Override
   	protected boolean isSplitable(JobContext context, Path filename) {
   		return false;
   	}
   
   	@Override
   	public RecordReader<Text, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context)
   			throws IOException, InterruptedException {
   
   		WholeRecordReader recordReader = new WholeRecordReader();
   		recordReader.initialize(split, context);
   
   		return recordReader;
   	}
   }
   ```

   

#### 3.2.3 SequenceFileMapperç±»

1. ç±»è¯´æ˜ï¼šä»¥æ–‡ä»¶åæœ€ä¸ºKeyï¼Œæ–‡ä»¶å†…å®¹ä½œä¸ºValueã€‚æä¾›ç»™reducer

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.inputformat;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.BytesWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Mapper;
   
   public class SequenceFileMapper extends Mapper<Text, BytesWritable, Text, BytesWritable> {
   
   	@Override
   	protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException {
   
   		context.write(key, value);
   	}
   }
   ```

   

#### 3.2.4 SequenceFileReducerç±»

1. ç±»è¯´æ˜ï¼šå°†keyå’Œvalueå†™å‡ºåˆ°æœ€ç»ˆè¾“å‡º

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.inputformat;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.BytesWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   
   public class SequenceFileReducer extends Reducer<Text, BytesWritable, Text, BytesWritable> {
   
   	@Override
   	protected void reduce(Text key, Iterable<BytesWritable> values, Context context)
   			throws IOException, InterruptedException {
   
   		context.write(key, values.iterator().next());
   	}
   }
   ```

   

#### 3.2.5 SequeceFileDriverç±»

1. ç±»è¯´æ˜ï¼š

   - è®¾ç½®è¾“å…¥çš„inputFormatç±»å‹ï¼šjob.setInputFormatClass(WholeFileInputformat.class);
   - è®¾ç½®è¾“å‡ºçš„outputFormatç±»å‹ï¼šjob.setOutputFormatClass(SequenceFileOutputFormat.class);

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.inputformat;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.BytesWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
   
   public class SequenceFileDriver {
   
   	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
   
   		// è¾“å…¥è¾“å‡ºè·¯å¾„éœ€è¦æ ¹æ®è‡ªå·±ç”µè„‘ä¸Šå®é™…çš„è¾“å…¥è¾“å‡ºè·¯å¾„è®¾ç½®
   		args = new String[] { "e:/input/inputFormatInput", "e:/output/inputFormatOutput" };
   
   		// 1 è·å–jobå¯¹è±¡
   		Configuration conf = new Configuration();
   		Job job = Job.getInstance(conf);
   
   		// 2 è®¾ç½®jaråŒ…å­˜å‚¨ä½ç½®ã€å…³è”è‡ªå®šä¹‰çš„mapperå’Œreducer
   		job.setJarByClass(SequenceFileDriver.class);
   		job.setMapperClass(SequenceFileMapper.class);
   		job.setReducerClass(SequenceFileReducer.class);
   
   		// 7è®¾ç½®è¾“å…¥çš„inputFormat
   		job.setInputFormatClass(WholeFileInputformat.class);
   
   		// 8è®¾ç½®è¾“å‡ºçš„outputFormat
   		job.setOutputFormatClass(SequenceFileOutputFormat.class);
   
   // 3 è®¾ç½®mapè¾“å‡ºç«¯çš„kvç±»å‹
   		job.setMapOutputKeyClass(Text.class);
   		job.setMapOutputValueClass(BytesWritable.class);
   
   		// 4 è®¾ç½®æœ€ç»ˆè¾“å‡ºç«¯çš„kvç±»å‹
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(BytesWritable.class);
   
   		// 5 è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 6 æäº¤job
   		boolean result = job.waitForCompletion(true);
   		System.exit(result ? 0 : 1);
   	}
   }
   ```



# äº”ã€ Partitionåˆ†åŒºï¼šå°†ç»“æœè¾“å‡ºåˆ°å¤šä¸ªæ–‡ä»¶ä¸­

## 1. å®éªŒä»‹ç»

### 1.1 å®éªŒè¾“å…¥

```
1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
```



### 1.2 æœŸå¾…è¾“å‡º

```
13470253144	180	180	360
13509468723	7335	110349	117684
13560439638	918	4938	5856
13568436656	3597	25635	29232
13590439668	1116	954	2070
13630577991	6960	690	7650
13682846555	1938	2910	4848
13729199489	240	0	240
13736230513	2481	24681	27162
13768778790	120	120	240
13846544121	264	0	264
13956435636	132	1512	1644
13966251146	240	0	240
13975057813	11058	48243	59301
13992314666	3008	3720	6728
15043685818	3659	3538	7197
15910133277	3156	2936	6092
15959002129	1938	180	2118
18271575951	1527	2106	3633
18390173782	9531	2412	11943
84188413	4116	1432	5548
```

åº”è¯¥æœ‰5ä¸ªè¾“å‡ºæ–‡ä»¶ï¼š

![image-20200419155255436](.\02-mapreduce-experiments.assets\image-20200419155255436.png)

### 1.3 å®éªŒè¯´æ˜

è¯»å–æ‰‹æœºçš„æµé‡ä¿¡æ¯ï¼Œç„¶åè¾“å‡ºæ‰‹æœºçš„ä¸Šè¡Œæµé‡ã€ä¸‹è¡Œæµé‡å’Œæ€»æµé‡

æ‰‹æœºå·136ã€137ã€138ã€139å¼€å¤´éƒ½åˆ†åˆ«æ”¾åˆ°ä¸€ä¸ªç‹¬ç«‹çš„4ä¸ªæ–‡ä»¶ä¸­ï¼Œå…¶ä»–å¼€å¤´çš„æ”¾åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚

## 2. å®éªŒä»£ç 

### 2.1 ProvincePartitionerç±»

1. ç±»è¯´æ˜ï¼šæˆ‘ä»¬åœ¨FlowSumå®éªŒçš„åŸºç¡€ä¸Šæ·»åŠ äº†ProvincePartitionerç±»ï¼Œç”¨æ¥åŒºåˆ†ä¸åŒçš„æ‰‹æœºå·ã€‚ç»§æ‰¿Partitionerç±»ï¼Œé‡å†™getPartitionæ–¹æ³•

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.partition;
   
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Partitioner;
   
   public class ProvincePartitioner extends Partitioner<Text, FlowBean> {
   
   	@Override
   	public int getPartition(Text key, FlowBean value, int numPartitions) {
   
   		// 1. Get first 3 numbers of phone number
   		String preNum = key.toString().substring(0, 3);
   
   		int partition = 4;
   		// 2. Verify province
   		if ("136".equals(preNum)) {
   			partition = 0;
   		} else if ("137".equals(preNum)) {
   			partition = 1;
   		} else if ("138".equals(preNum)) {
   			partition = 2;
   		} else if ("139".equals(preNum)) {
   			partition = 3;
   		}
   		return partition;
   	}
   }
   ```

### 2.2 FlowsumDriverç±»

1. ç±»è¯´æ˜ï¼šå¢åŠ è‡ªå®šä¹‰æ•°æ®åˆ†åŒºè®¾ç½®å’ŒReduceTaskè®¾ç½®

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.partition;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   
   public class FlowsumDriver {
   
   	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
   		// Set input path and output path
   		args = new String[] { "e:/input/partition", "e:/output/partition" };
   
   		// 1. Get configuration information, or job object instance
   		Configuration conf = new Configuration();
   		Job job = Job.getInstance(conf);
   
   		// 2. Specify the local path where the jar package of this program is located
   		job.setJarByClass(FlowsumDriver.class);
   
   		// 3. Specify the mapper class and the reducer class
   		job.setMapperClass(FlowCountMapper.class);
   		job.setReducerClass(FlowCountReducer.class);
   
   		// 4. Specify map output key type and value type
   		job.setMapOutputKeyClass(Text.class);
   		job.setMapOutputValueClass(FlowBean.class);
   
   		// 5. Specify final output key type and value type
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(FlowBean.class);
   
   		// 8 æŒ‡å®šè‡ªå®šä¹‰æ•°æ®åˆ†åŒº
   		job.setPartitionerClass(ProvincePartitioner.class);
   
   		// 9 åŒæ—¶æŒ‡å®šç›¸åº”æ•°é‡çš„reduce task
   		job.setNumReduceTasks(5);
   
   		// 6. Specify the directory where the original input file of the job is located
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 7. Submit
   		Boolean result = job.waitForCompletion(true);
   		System.exit(result ? 0 : 1);
   	}
   }
   
   ```

# å…­ã€ æ’åº

1. mapçš„æ’åºè¿‡ç¨‹ï¼š

   - é»˜è®¤æ’åºæ˜¯æŒ‰ç…§å­—å…¸é¡ºåºæ’åºï¼Œä¸”å®ç°è¯¥æ’åºçš„æ–¹æ³•æ˜¯å¿«é€Ÿæ’åº
   - å¯¹äºMapTaskï¼Œå®ƒä¼šå°†å¤„ç†çš„ç»“æœæš‚æ—¶æ”¾åˆ°ç¯å½¢ç¼“å†²åŒºä¸­ï¼Œå½“ç¯å½¢ç¼“å†²åŒºä½¿ç”¨ç‡è¾¾åˆ°ä¸€å®šé˜ˆå€¼åï¼Œå†å¯¹ç¼“å†²åŒºä¸­çš„æ•°æ®è¿›è¡Œä¸€æ¬¡å¿«é€Ÿæ’åºï¼Œå¹¶å°†è¿™äº›æœ‰åºæ•°æ®æº¢å†™åˆ°ç£ç›˜ä¸Šï¼Œè€Œå½“æ•°æ®å¤„ç†å®Œæ¯•åï¼Œå®ƒä¼šå¯¹ç£ç›˜ä¸Šæ‰€æœ‰æ–‡ä»¶è¿›è¡Œå½’å¹¶æ’åºã€‚

2. reduceæ’åºè¿‡ç¨‹ï¼š

   å¯¹äºReduceTaskï¼Œå®ƒä»æ¯ä¸ªMapTaskä¸Šè¿œç¨‹æ‹·è´ç›¸åº”çš„æ•°æ®æ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶å¤§å°è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼Œåˆ™æº¢å†™ç£ç›˜ä¸Šï¼Œå¦åˆ™å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚å¦‚æœç£ç›˜ä¸Šæ–‡ä»¶æ•°ç›®è¾¾åˆ°ä¸€å®šé˜ˆå€¼ï¼Œåˆ™è¿›è¡Œä¸€æ¬¡å½’å¹¶æ’åºä»¥ç”Ÿæˆä¸€ä¸ªæ›´å¤§æ–‡ä»¶ï¼›å¦‚æœå†…å­˜ä¸­æ–‡ä»¶å¤§å°æˆ–è€…æ•°ç›®è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼Œåˆ™è¿›è¡Œä¸€æ¬¡åˆå¹¶åå°†æ•°æ®æº¢å†™åˆ°ç£ç›˜ä¸Šã€‚å½“æ‰€æœ‰æ•°æ®æ‹·è´å®Œæ¯•åï¼ŒReduceTaskç»Ÿä¸€å¯¹å†…å­˜å’Œç£ç›˜ä¸Šçš„æ‰€æœ‰æ•°æ®è¿›è¡Œä¸€æ¬¡å½’å¹¶æ’åºã€‚



## 1. FlowCountSortæ’åºæ¡ˆä¾‹

### 1.1 å®éªŒä»‹ç»

#### 1.1.1 å®éªŒè¾“å…¥

```
13470253144	180	180	360
13509468723	7335	110349	117684
13560439638	918	4938	5856
13568436656	3597	25635	29232
13590439668	1116	954	2070
13630577991	6960	690	7650
13682846555	1938	2910	4848
13729199489	240	0	240
13736230513	2481	24681	27162
13768778790	120	120	240
13846544121	264	0	264
13956435636	132	1512	1644
13966251146	240	0	240
13975057813	11058	48243	59301
13992314666	3008	3720	6728
15043685818	3659	3538	7197
15910133277	3156	2936	6092
15959002129	1938	180	2118
18271575951	1527	2106	3633
18390173782	9531	2412	11943
84188413	4116	1432	5548
```



#### 1.1.2 æœŸå¾…è¾“å‡º

```
c
```



#### 1.1.3 å®éªŒè¯´æ˜

æ ¹æ®æ¡ˆä¾‹2.3 Flowsumäº§ç”Ÿçš„ç»“æœå†æ¬¡å¯¹æ€»æµé‡è¿›è¡Œæ’åº

### 1.2 å®éªŒä»£ç 

#### 2.1.1 FlowBeanç±»

1. ç±»ä»‹ç»ï¼šåœ¨åŸå§‹çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†æ¯”è¾ƒéƒ¨åˆ†compareTo

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.sort;
   
   import java.io.DataInput;
   import java.io.DataOutput;
   import java.io.IOException;
   
   import org.apache.hadoop.io.WritableComparable;
   
   // 1. Implement the writable interface
   public class FlowBean implements WritableComparable<FlowBean> {
   
   	private long upFlow;
   	private long downFlow;
   	private long sumFlow;
   
   	// When deserializing, you need to call the empty parameter constructor, so you
   	// must have
   	public FlowBean() {
   		super();
   	}
   
   	public FlowBean(long upFlow, long downFlow) {
   		super();
   		this.upFlow = upFlow;
   		this.downFlow = downFlow;
   		this.sumFlow = upFlow + downFlow;
   	}
   
   	// Write serialization method
   	public void write(DataOutput out) throws IOException {
   		out.writeLong(upFlow);
   		out.writeLong(downFlow);
   		out.writeLong(sumFlow);
   	}
   
   	// Deserialization method
   	// The read sequence of the deserialization method must be the same as the write
   	// sequence of the write serialization method
   	public void readFields(DataInput in) throws IOException {
   		this.upFlow = in.readLong();
   		this.downFlow = in.readLong();
   		this.sumFlow = in.readLong();
   	}
   
   	// Write toString method to facilitate subsequent printing to text
   	@Override
   	public String toString() {
   		return upFlow + "\t" + downFlow + "\t" + sumFlow;
   	}
   
   	// set and get methods
   	public long getUpFlow() {
   		return upFlow;
   	}
   
   	public void setUpFlow(long upFlow) {
   		this.upFlow = upFlow;
   	}
   
   	public long getDownFlow() {
   		return downFlow;
   	}
   
   	public void setDownFlow(long downFlow) {
   		this.downFlow = downFlow;
   	}
   
   	public long getSumFlow() {
   		return sumFlow;
   	}
   
   	public void setSumFlow(long sumFlow) {
   		this.sumFlow = sumFlow;
   	}
   
   	public void set(long upFlow2, long downFlow2) {
   		this.upFlow = upFlow2;
   		this.downFlow = downFlow2;
   		this.sumFlow = upFlow2 + downFlow2;
   	}
   
   	// Add compare function
   	public int compareTo(FlowBean bean) {
   
   		int result;
   		// According to the total flow size, in reverse order
   		if (sumFlow > bean.getSumFlow()) {
   			result = -1;
   		} else if (sumFlow < bean.getSumFlow()) {
   			result = 1;
   		} else {
   			result = 0;
   		}
   
   		return result;
   
   	}
   }
   ```

#### 1.2.2 FlowCountSortMapperç±»

1. ç±»ä»‹ç»ï¼š

   è¾“å‡ºkeyä¸ºBeanï¼Œå› ä¸ºæ’åºè¿‡ç¨‹æ˜¯ä¹Ÿkeyä½œä¸ºæ’åºçš„ï¼Œæ‰€ä»¥è¦æŠŠBeanä½œä¸ºkey

   è¾“å‡ºvalueä¸ºphoneNumber

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.sort;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Mapper;
   
   public class FlowCountSortMapper extends Mapper<LongWritable, Text, FlowBean, Text> {
   
   	FlowBean bean = new FlowBean();
   	Text v = new Text();
   
   	@Override
   	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
   
   		// 1 è·å–ä¸€è¡Œ
   		String line = value.toString();
   
   		// 2 æˆªå–
   		String[] fields = line.split("\t");
   
   		// 3 å°è£…å¯¹è±¡
   		String phoneNbr = fields[0];
   		long upFlow = Long.parseLong(fields[1]);
   		long downFlow = Long.parseLong(fields[2]);
   
   		bean.set(upFlow, downFlow);
   		v.set(phoneNbr);
   
   		// 4 è¾“å‡º
   		context.write(bean, v);
   	}
   }
   ```

   

#### 1.2.3 FlowCountSortReducerç±»

1. ç±»ä»‹ç»ï¼š

   è¾“å‡ºç»“æœkeyä¸ºæ‰‹æœºå·

   è¾“å‡ºç»“æœvalueä¸ºBean

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.sort;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   
   public class FlowCountSortReducer extends Reducer<FlowBean, Text, Text, FlowBean> {
   
   	@Override
   	protected void reduce(FlowBean key, Iterable<Text> values, Context context)
   			throws IOException, InterruptedException {
   
   		// å¾ªç¯è¾“å‡ºï¼Œé¿å…æ€»æµé‡ç›¸åŒæƒ…å†µ
   		for (Text text : values) {
   			context.write(text, key);
   		}
   	}
   }
   ```



## 2. WordcountCombineræ¡ˆä¾‹ï¼šåŒºå†…æ’åº

### 2.1 å®éªŒä»‹ç»

#### 2.1.1 å®éªŒè¾“å…¥

```
13470253144	180	180	360
13509468723	7335	110349	117684
13560439638	918	4938	5856
13568436656	3597	25635	29232
13590439668	1116	954	2070
13630577991	6960	690	7650
13682846555	1938	2910	4848
13729199489	240	0	240
13736230513	2481	24681	27162
13768778790	120	120	240
13846544121	264	0	264
13956435636	132	1512	1644
13966251146	240	0	240
13975057813	11058	48243	59301
13992314666	3008	3720	6728
15043685818	3659	3538	7197
15910133277	3156	2936	6092
15959002129	1938	180	2118
18271575951	1527	2106	3633
18390173782	9531	2412	11943
84188413	4116	1432	5548
```



#### 2.1.2 æœŸå¾…è¾“å‡º

```
13470253144	180	180	360
13509468723	7335	110349	117684
13560439638	918	4938	5856
13568436656	3597	25635	29232
13590439668	1116	954	2070
13630577991	6960	690	7650
13682846555	1938	2910	4848
13729199489	240	0	240
13736230513	2481	24681	27162
13768778790	120	120	240
13846544121	264	0	264
13956435636	132	1512	1644
13966251146	240	0	240
13975057813	11058	48243	59301
13992314666	3008	3720	6728
15043685818	3659	3538	7197
15910133277	3156	2936	6092
15959002129	1938	180	2118
18271575951	1527	2106	3633
18390173782	9531	2412	11943
84188413	4116	1432	5548
```

æœŸå¾…çœ‹åˆ°å¤šä¸ªè¾“å‡ºæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶å†…éƒ½æ˜¯æœ‰åºçš„

#### 2.1.3 å®éªŒè¯´æ˜

åœ¨ä¸Šä¸€ä¸ªå®éªŒçš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº†è‡ªå®šä¹‰åˆ†åŒºç±»

åœ¨Driverä¸­è®¾ç½®äº†è‡ªå®šä¹‰åˆ†åŒºç±»å’ŒReducetaskçš„ä¸ªæ•°

### 2.2 å®éªŒä»£ç 

#### 2.2.1 ProvincePartitionerç±»

1. ç±»è¯´æ˜ï¼šextends Partitioner<FlowBean, Text>

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.partitionSort;
   
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Partitioner;
   
   public class ProvincePartitioner extends Partitioner<FlowBean, Text> {
   
   	@Override
   	public int getPartition(FlowBean key, Text value, int numPartitions) {
   
   		// 1 è·å–æ‰‹æœºå·ç å‰ä¸‰ä½
   		String preNum = value.toString().substring(0, 3);
   
   		int partition = 4;
   
   		// 2 æ ¹æ®æ‰‹æœºå·å½’å±åœ°è®¾ç½®åˆ†åŒº
   		if ("136".equals(preNum)) {
   			partition = 0;
   		} else if ("137".equals(preNum)) {
   			partition = 1;
   		} else if ("138".equals(preNum)) {
   			partition = 2;
   		} else if ("139".equals(preNum)) {
   			partition = 3;
   		}
   
   		return partition;
   	}
   }
   ```

   

#### 2.2.2 FlowCountSortDriverç±»

1. ç±»è¯´æ˜ï¼šè®¾ç½®äº†è‡ªå®šä¹‰åˆ†åŒºç±»å’ŒReducetaskçš„ä¸ªæ•°

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.partitionSort;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   
   public class FlowCountSortDriver {
   
   	public static void main(String[] args) throws ClassNotFoundException, IOException, InterruptedException {
   
   		// è¾“å…¥è¾“å‡ºè·¯å¾„éœ€è¦æ ¹æ®è‡ªå·±ç”µè„‘ä¸Šå®é™…çš„è¾“å…¥è¾“å‡ºè·¯å¾„è®¾ç½®
   		args = new String[] { "e:/output/flowSum", "e:/output/partitionSort" };
   
   		// 1 è·å–é…ç½®ä¿¡æ¯ï¼Œæˆ–è€…jobå¯¹è±¡å®ä¾‹
   		Configuration configuration = new Configuration();
   		Job job = Job.getInstance(configuration);
   
   		// 2 æŒ‡å®šæœ¬ç¨‹åºçš„jaråŒ…æ‰€åœ¨çš„æœ¬åœ°è·¯å¾„
   		job.setJarByClass(FlowCountSortDriver.class);
   
   		// 3 æŒ‡å®šæœ¬ä¸šåŠ¡jobè¦ä½¿ç”¨çš„mapper/Reducerä¸šåŠ¡ç±»
   		job.setMapperClass(FlowCountSortMapper.class);
   		job.setReducerClass(FlowCountSortReducer.class);
   
   		// 4 æŒ‡å®šmapperè¾“å‡ºæ•°æ®çš„kvç±»å‹
   		job.setMapOutputKeyClass(FlowBean.class);
   		job.setMapOutputValueClass(Text.class);
   
   		// 5 æŒ‡å®šæœ€ç»ˆè¾“å‡ºçš„æ•°æ®çš„kvç±»å‹
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(FlowBean.class);
   
   		// åŠ è½½è‡ªå®šä¹‰åˆ†åŒºç±»
   		job.setPartitionerClass(ProvincePartitioner.class);
   
   		// è®¾ç½®Reducetaskä¸ªæ•°
   		job.setNumReduceTasks(5);
   
   		// 6 æŒ‡å®šjobçš„è¾“å…¥åŸå§‹æ–‡ä»¶æ‰€åœ¨ç›®å½•
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 7 å°†jobä¸­é…ç½®çš„ç›¸å…³å‚æ•°ï¼Œä»¥åŠjobæ‰€ç”¨çš„javaç±»æ‰€åœ¨çš„jaråŒ…ï¼Œ æäº¤ç»™yarnå»è¿è¡Œ
   		boolean result = job.waitForCompletion(true);
   		System.exit(result ? 0 : 1);
   	}
   }
   ```



# ä¸ƒã€ Conbineråˆå¹¶ï¼šå°†mapè¾“å‡ºè¿›è¡Œåˆå¹¶ï¼Œä»è€Œå‡å°ç½‘ç»œä¼ è¾“é‡

## 

## 1. è¯•éªŒä»‹ç»

### 1.1 å®éªŒè¾“å…¥

```
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
```



### 1.2 æœŸå¾…è¾“å‡º

```
banzhang	4
hadoop	2
hao	2
ni	2
xihuan	2
```

å¯ä»¥çœ‹åˆ°combinerå‘ç”Ÿäº†ä½œç”¨ï¼Œæœ¬æ¥mapçš„è¾“å‡ºç”±12ä¸ªå˜æˆäº†5ä¸ª

![image-20200421161906124](.\02-mapreduce-experiments.assets\image-20200421161906124.png)

### 1.3 å®éªŒè¯´æ˜

å¢åŠ ä¸€ä¸ªWordcountCombinerç±»ç»§æ‰¿Reducer

## 2. å®éªŒä»£ç 

### 2.1 WordcountCombinerç±»

1. ç±»è¯´æ˜ï¼šè¿™ä¸ªç±»å’ŒReducerç±»å¾ˆåƒï¼Œå°±æ˜¯åšäº†æ±‡æ€»çš„å·¥ä½œ

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.combier;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.IntWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   
   public class WordcountCombiner extends Reducer<Text, IntWritable, Text, IntWritable> {
   
   	IntWritable v = new IntWritable();
   
   	@Override
   	protected void reduce(Text key, Iterable<IntWritable> values, Context context)
   			throws IOException, InterruptedException {
   		// 1. Summarize
   		int sum = 0;
   		for (IntWritable value : values) {
   			sum += value.get();
   		}
   		v.set(sum);
   
   		// 2. Write out
   		context.write(key, v);
   	}
   }
   
   ```

### 2.2 WordcountDriverç±»

1. ç±»è¯´æ˜ï¼šæŒ‡å®šäº†CombinerClassï¼Œç›´æ¥æŒ‡å®šCombinerClassä¸ºReducerç±»ä¹Ÿæ˜¯å¯ä»¥çš„

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.combier;
   
   import java.io.IOException;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.IntWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   
   /**
    * Driver will configure yarn
    * 
    * 1. Get job object
    * 
    * 2. Specify jar class, Mapper class and Reducer class (3)
    * 
    * 3. Specify Mapper input and output class and Final input and output class (2)
    * 
    * 4. Submit (1)
    */
   public class WordcountDriver {
   
   	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
   
   		args = new String[] { "e:/input/combiner", "e:/output/combiner" };
   		// 1. Obtain configuration information and packaging tasks
   		Configuration conf = new Configuration();
   		Job job = Job.getInstance(conf);
   
   		// 2. Set jar loading path
   		job.setJarByClass(WordcountDriver.class);
   
   		// 3. set map and reduce class
   		job.setMapperClass(WordcountMapper.class);
   		job.setReducerClass(WordcountReducer.class);
   
   		// 4. set map output
   		job.setMapOutputKeyClass(Text.class);
   		job.setOutputValueClass(IntWritable.class);
   
   		// 5. set final k and v output type
   		job.setOutputKeyClass(Text.class);
   		job.setOutputValueClass(IntWritable.class);
   
   		// 6. set input path and output path
   		FileInputFormat.setInputPaths(job, new Path(args[0]));
   		FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
   		// 8. set combiner class
   		job.setCombinerClass(WordcountCombiner.class);
   
   		// 7. submit
   		boolean result = job.waitForCompletion(true);
   		System.exit(result ? 0 : 1);
   	}
   
   }
   ```

# å…«ã€ GroupingComparatoråˆ†ç»„ï¼ˆè¾…åŠ©æ’åºï¼‰

## 1. è¯•éªŒä»‹ç»

### 1.1 å®éªŒè¾“å…¥

```
0000001	Pdt_01	222.8
0000002	Pdt_05	722.4
0000001	Pdt_02	33.8
0000003	Pdt_06	232.8
0000003	Pdt_02	33.8
0000002	Pdt_03	522.8
0000002	Pdt_04	122.4
```



### 1.2 æœŸå¾…è¾“å‡º

```
1	222.8
2	722.4
3	232.8
```



### 1.3 å®éªŒè¯´æ˜

ï¼ˆ1ï¼‰åˆ©ç”¨â€œè®¢å•idå’Œæˆäº¤é‡‘é¢â€ä½œä¸ºkeyï¼Œå¯ä»¥å°†Mapé˜¶æ®µè¯»å–åˆ°çš„æ‰€æœ‰è®¢å•æ•°æ®æŒ‰ç…§idå‡åºæ’åºï¼Œå¦‚æœidç›¸åŒå†æŒ‰ç…§é‡‘é¢é™åºæ’åºï¼Œå‘é€åˆ°Reduceã€‚

ï¼ˆ2ï¼‰åœ¨Reduceç«¯åˆ©ç”¨groupingComparatorå°†è®¢å•idç›¸åŒçš„kvèšåˆæˆç»„ï¼Œç„¶åå–ç¬¬ä¸€ä¸ªå³æ˜¯è¯¥è®¢å•ä¸­æœ€è´µå•†å“ï¼Œå¦‚å›¾4-18æ‰€ç¤ºã€‚

## 2. å®éªŒä»£ç 

### 2.1 OrderBeanç±»

1. ç±»è¯´æ˜ï¼šå®šä¹‰è®¢å•ä¿¡æ¯

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.order;
   
import java.io.DataInput;
   import java.io.DataOutput;
   import java.io.IOException;
   
   import org.apache.hadoop.io.WritableComparable;
   
   public class OrderBean implements WritableComparable<OrderBean> {
   
   	private int order_id; // è®¢å•idå·
   	private double price; // ä»·æ ¼
   
   	public OrderBean(int order_id, double price) {
   		super();
   		this.order_id = order_id;
   		this.price = price;
   	}
   
   	public OrderBean() {
   		super();
   	}
   
   	public void write(DataOutput out) throws IOException {
   		out.writeInt(order_id);
   		out.writeDouble(price);
   	}
   
   	public void readFields(DataInput in) throws IOException {
   		order_id = in.readInt();
   		price = in.readDouble();
   	}
   
   	// sort
   	public int compareTo(OrderBean o) {
   		int result;
   
   		if (order_id > o.getOrder_id()) {
   			result = 1;
   		} else if (order_id < o.getOrder_id()) {
   			result = -1;
   		} else {
   			// Secondary sort
   			result = price > o.getPrice() ? -1 : 1;
   		}
   
   		return result;
   
   	}
   
   	@Override
   	public String toString() {
   		return order_id + "\t" + price;
   	}
   
   	public int getOrder_id() {
   		return order_id;
   	}
   
   	public void setOrder_id(int order_id) {
   		this.order_id = order_id;
   	}
   
   	public double getPrice() {
   		return price;
   	}
   
   	public void setPrice(double price) {
   		this.price = price;
   	}
   
   }
   ```
   

### 2.2 OrderGroupingComparatorç±»

1. ç±»è¯´æ˜ï¼šå®šä¹‰äº†æ’åºæ—¶æ¯”è¾ƒçš„æ–¹æ³•

2. ç±»ä»£ç ï¼š

   ```java
   package com.fujie.mapreduce.order;
   
   import org.apache.hadoop.io.WritableComparable;
   import org.apache.hadoop.io.WritableComparator;
   
   public class OrderGroupingComparator extends WritableComparator {
   
   	protected OrderGroupingComparator() {
   		super(OrderBean.class, true);
   	}
   
   	@Override
   	public int compare(WritableComparable a, WritableComparable b) {
   
   		OrderBean aBean = (OrderBean) a;
   		OrderBean bBean = (OrderBean) b;
   
   		int result;
   		if (aBean.getOrder_id() > bBean.getOrder_id()) {
   			result = 1;
   		} else if (aBean.getOrder_id() < bBean.getOrder_id()) {
   			result = -1;
   		} else {
   			result = 0;
   		}
   
   		return result;
   	}
   }
   ```

   

# ä¹ã€ OutputFormatå®éªŒ









